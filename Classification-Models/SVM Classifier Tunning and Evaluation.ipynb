{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SUPPORT VECTOR MACHINE Classifier Tunning and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tunning the classifier according the number of features, feature selection method, rescaling strategy, and hyperparamters set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the data (.csv file)\n",
    "\n",
    "data_raw = pd.read_csv('3DRadiomics.csv')\n",
    "data = data_raw.reindex(np.random.permutation(data_raw.index))\n",
    "\n",
    "print (\"Dataset read OK!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Coding binary features -> Outcome:\n",
    "\n",
    "'''\n",
    "Immuno -> 0 = False = Negative for any hormonone\n",
    "         1 = True = Positive for one or more hormones (ACTH, LH, GH, PRL, FSH)\n",
    "Hardy -> 0 = False = No sign of invasiveness\n",
    "         1 = True = Some level of invasiveness\n",
    "Progression -> 0 = False = Stable lesion in the observed period\n",
    "               1 = True = Recurrent lesion in the observed period\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organizing and separating features and classes\n",
    "\n",
    "# Catching dataset without 'Patients' Column\n",
    "data_new = data.drop(['Patients'], axis=1).copy()\n",
    "features = data_new.drop(['Status'], axis=1).copy()\n",
    "\n",
    "# Converting dataset into float array\n",
    "data_np = data_new.astype(np.float64).values\n",
    "\n",
    "# Catching all line values except the last column (ends to be a matrix) - samples matrix\n",
    "X_raw = data_np[:,:-1] \n",
    "\n",
    "# Catching only the last y-component's tuple (vector, or column) - target column\n",
    "y = data_np[:,-1]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dictionary to hold names in our calssification\n",
    "--->> Stable == 0 == False; Recurrent == 1 == True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used in the confusion matrix\n",
    "\n",
    "classes = ['Stable','Recurrent']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rescaling routine and function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing.data import QuantileTransformer\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn import preprocessing as prep \n",
    "MinMaxScaler = prep.MinMaxScaler((0,1))\n",
    "\n",
    "def rescaling(normChosen,X_r0):\n",
    "    if normChosen == 0: # Robust Scalar\n",
    "        X_r1 = RobustScaler(quantile_range=(25, 75)).fit_transform(X_r0)\n",
    "        return X_r1\n",
    "    elif normChosen == 1: # Standard Scalar\n",
    "        X_r1 = StandardScaler().fit_transform(X_r0)\n",
    "        return X_r1\n",
    "    elif normChosen == 2: # Quantile Transformer Scaler        \n",
    "        X_r1 = QuantileTransformer(output_distribution='uniform').fit_transform(X_r0)\n",
    "        return X_r1\n",
    "    elif normChosen == 3: # Normalizer        \n",
    "        X_r1 = Normalizer().fit_transform(X_r0)\n",
    "        return X_r1\n",
    "    elif normChosen == 5: # Scale        \n",
    "        X_r1 = scale(X_r0)\n",
    "        return X_r1\n",
    "    elif normChosen == 6: # MinMaxScaler        \n",
    "        X_r1 = MinMaxScaler.fit_transform(X_r0)\n",
    "        return X_r1\n",
    "    else:\n",
    "        return X_r0\n",
    "print (\"Reascaling function OK!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Feature selection function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier # extremely randomized clf\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def featureSelection(featSelChosen,X_f0,size):\n",
    "    if featSelChosen == 0: # ExtraTreeClass Feat. Sel.\n",
    "        clfRF = ExtraTreesClassifier()\n",
    "        clfRF = clfRF.fit(X_f0, y)\n",
    "        clfRF.feature_importances_\n",
    "        model = SelectFromModel(clfRF, prefit=True)\n",
    "        X_f1 = model.transform(X_f0)\n",
    "        return X_f1\n",
    "    elif featSelChosen == 1: # Select K best feat.\n",
    "        select = SelectKBest(mutual_info_classif, k=size)\n",
    "        X_f1 = select.fit_transform(X_f0, y)\n",
    "        mask = select.get_support()\n",
    "        return X_f1,mask\n",
    "    elif featSelChosen == 2: # Principal Component Analysis\n",
    "        pca = PCA(n_components=size,svd_solver='full')\n",
    "        pca.fit(X_f0,y)\n",
    "        X_f1 = pca.transform(X_f0)\n",
    "        return X_f1\n",
    "    elif fsChosen == 3: # ReliefF feature selection filrer\n",
    "        reliefF = ReliefF(n_neighbors=10, n_features_to_keep=size)\n",
    "        X_f1 = reliefF.fit_transform(X_f0, y)\n",
    "        return X_f1\n",
    "print (\"Feature selection funtion OK!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing step caller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preProcessX(size):\n",
    "    '''\n",
    "        'size' indicates the number of desired selected features in the feature\n",
    "        selection step.\n",
    "                \n",
    "        'typeChosen' indicates standardization strategy:\n",
    "            #0: Robust Scalar\n",
    "            #1: Standard Scalar\n",
    "            #2: Quantile Transformer (uniform) Scaler\n",
    "            #3: Normalizer\n",
    "            #4: No normalization\n",
    "            #5: Scale\n",
    "            #6: MinMax Scaller\n",
    "        \n",
    "        'fsChosen' indicates features selection strategy:\n",
    "            #0: ExtraTreeClass Feat. Sel.\n",
    "            #1: Select K best feat.\n",
    "            #2: Principal Component Analysis\n",
    "            #3: ReliefF feature selection filrer\n",
    "    '''\n",
    "    \n",
    "    normChosen = 6\n",
    "    featSelChosen = 1\n",
    "    # X_sub = subset of relevant features\n",
    "    X_sub, mask = featureSelection(featSelChosen,X_raw,size) \n",
    "    # X_resc = rescaled subset of relevant features\n",
    "    X_norm = rescaling(normChosen,X_sub)                   \n",
    "    # print (X_sub.shape) \n",
    "    \n",
    "    # Returning \n",
    "    return X_norm, X_sub, mask\n",
    "print (\"Preprocess caller funtion OK!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusiom Matrix function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# cm = Confusion matrix; classes = array holding class names; cmap = color scheme; \n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title,fontsize=16)\n",
    "    #plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, fontsize = 12) # rotation=45, this parameter might be added to rotate the labels. \n",
    "    plt.yticks(tick_marks, classes, fontsize = 12, rotation = 90, verticalalignment=\"center\")\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\",fontsize=16)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label',fontsize=16)\n",
    "    plt.xlabel('Predicted label',fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    name = title + '.png'\n",
    "    plt.savefig(name, dpi=300)\n",
    "    plt.show()\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAriable that hold all the \n",
    "final_result = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, test, and evaluate classifiers for a given parameter set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import roc_curve\n",
    "from scipy import interp\n",
    "\n",
    "def run(params):\n",
    "\n",
    "    size = params[0]\n",
    "    execution_time = params[1]\n",
    "    classifier = SVC(kernel=params[2], C=params[3], gamma=params[4], probability=True)\n",
    "    classifier_name = \"SVM\"\n",
    "\n",
    "    X,X_sub, mask = preProcessX(size)\n",
    "    # X is the normalized subset, ready for processing\n",
    "    # X_sub is the raw subset used only for identifying the used features by name\n",
    "    \n",
    "    # Definig parameters for k-fold cross validation\n",
    "    # n_samples = len(X) # Number of samples in our dataset -> Same as LOOCV\n",
    "    kf = KFold(n_splits=3)\n",
    "                \n",
    "    # Concatenating both X and y data into a single array in order to use kFold-cv!\n",
    "    dataset = np.column_stack([X,y])\n",
    "    \n",
    "    tprs = []  # hold the true positive rate for each pair test-train \n",
    "    aucs = []  # hold the area under the curve for each pair test-train\n",
    "    spes = []  # hold the specificity for each pair test-train\n",
    "    sens = []  # hold the sensitivity rate for each pair test-train\n",
    "    accs = []  # hold the accuracy rate for each pair test-train\n",
    "    \n",
    "    mean_fpr = np.linspace(0, 1, 100)\n",
    "    \n",
    "    for train_indices, test_indices in kf.split(dataset):\n",
    "\n",
    "        # The output resultant refers to the sample indexes present in the\n",
    "        # [Training set] and [Testing set] respectively!\n",
    "\n",
    "        X_train = X[train_indices]     # X array hold only sample features. 'test' and 'train' bring a list of line numbers\n",
    "        y_train = y[train_indices]     # y array hold sample classification (1-d array) \n",
    "        X_test = X[test_indices]       # We have not explicitly\n",
    "        y_test = y[test_indices]       # Y-true for each pair train_test\n",
    "            \n",
    "        classifier.fit(X_train, y_train)\n",
    "        probas_ = classifier.predict_proba(X_test)\n",
    "        y_pred = classifier.predict(X_test)\n",
    "        fpr, tpr, thresholds = roc_curve(y_test, probas_[:, 1])\n",
    "\n",
    "        tprs.append(interp(mean_fpr, fpr, tpr))\n",
    "        tprs[-1][0] = 0.0\n",
    "\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        aucs.append(roc_auc)\n",
    "        \n",
    "        # Computing confusion matrix\n",
    "        cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "        # Classification numbers out of confusion matrix\n",
    "        tp, fn, fp, tn = float(cnf_matrix[1,1]), float(cnf_matrix[1,0]), float(cnf_matrix[0,1]), float(cnf_matrix[0,0])\n",
    "\n",
    "        # Computing scores\n",
    "        acc = (tp + tn)/(tp + fp + fn + tn)\n",
    "        sen = (tp)/(tp + fn)\n",
    "        spe = (tn)/(tn + fp)\n",
    "        \n",
    "        accs.append(acc)\n",
    "        sens.append(sen)\n",
    "        spes.append(spe)\n",
    "    \n",
    "    mean_tpr = np.mean(tprs, axis=0)\n",
    "    mean_tpr[-1] = 1.0\n",
    "    mean_auc = auc(mean_fpr, mean_tpr)\n",
    "    std_auc = np.std(aucs)\n",
    "    \n",
    "    accuracy = np.mean(accs, axis=0)\n",
    "    sensitivity = np.mean(sens, axis=0) \n",
    "    specificity = np.mean(spes, axis=0)\n",
    "        \n",
    "    # Identifying features by name\n",
    "    X_feature_names = features.columns[mask]\n",
    "    \n",
    "    clf_result = [accuracy, sensitivity, specificity, mean_auc, params, size, X_feature_names]\n",
    "    final_result.append(clf_result)\n",
    "    print(size,\"%.3f\" % accuracy,\"%.3f\" % specificity,\"%.3f\" % mean_auc)    \n",
    "    return accuracy, sensitivity, specificity, mean_auc, params, size, X_feature_names\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Solution assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best(results):\n",
    "    acc0, spe0, auc0, sen0, best_size = 0.0, 0.0, 0.0, 0.0, 0.0\n",
    "    for r in results:\n",
    "        accuracy, sensitivity, specificity, roc_auc, size = r[0], r[1], r[2], r[3], r[5]\n",
    "        if ((accuracy > acc0) or\n",
    "            (accuracy >= acc0 and specificity > spe0) or     \n",
    "            (accuracy == acc0 and specificity >= spe0 and roc_auc > auc0) or\n",
    "            (accuracy == acc0 and specificity == spe0 and roc_auc == auc0 and size < best_size)):\n",
    "   \n",
    "            best_X = r[6]\n",
    "            best_config = r[4]\n",
    "            best_size, acc0, sen0, spe0, auc0 = size, accuracy, sensitivity, specificity, roc_auc\n",
    "\n",
    "            print ('Better Result!')\n",
    "            print (best_size,\"%.3f\" % acc0,\"%.3f\" % spe0, \"%.3f\" % auc0)\n",
    "        else:\n",
    "            #print ('Not Good!')\n",
    "            print (size,\"%.3f\" % accuracy,\"%.3f\" % specificity, \"%.3f\" % roc_auc)\n",
    "\n",
    "    print(\"The very best solution is:\")\n",
    "    print(acc0, sen0, spe0, auc0)\n",
    "    print(\"features\",best_size,\": \", best_X)\n",
    "    print(\"Configuration: \",best_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel Execution Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool as ThreadPool\n",
    "\n",
    "def runParallel(parameters, threads=7):\n",
    "    pool = ThreadPool(threads)\n",
    "    results = pool.map(run, parameters)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Tunning Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# variable to hold all the classifiers configuration and performance\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    num_feat = np.arange(1,16,1)\n",
    "    # num_feature = number of desired features in the selected subset\n",
    "    execution = np.arange(1,101,1)\n",
    "    # execution = number of executions performed with the same num_feat\n",
    "      \n",
    "    # Parameters on SVM\n",
    "    kernel = ['rbf','linear']\n",
    "    C = [0.1, 1, 10, 100]\n",
    "    gamma = [100, 10, 1, 0.1, 0.01, 0.001, 0.0001]\n",
    "    gamma = [10]\n",
    "    \n",
    "    # Generating possible confgurations for MLP classifiers.\n",
    "    params = []\n",
    "    for n in num_feat:\n",
    "        for e in execution:\n",
    "            for k in kernel:\n",
    "                for c in C:\n",
    "                    for g in gamma:\n",
    "                        config = [n,e,k,c,g]\n",
    "                        params.append(config)\n",
    "    print(\"Here we go!\")\n",
    "\n",
    "    res = runParallel(params, 4)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_best(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
